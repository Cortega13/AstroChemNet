# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

### Purpose

AstroChemNet creates fast surrogate models for astrochemical networks, which are computationally expensive to simulate directly. These networks model the chemical evolution of astronomical environments (molecular clouds, star-forming regions, etc.) but face significant computational challenges:

- **Hundreds of chemical species** (333 in our case) with abundances spanning ~20 orders of magnitude (from 10^-20 to 1.0 relative to H nuclei)
- **Thousands of coupled ODEs** describing reaction rates, involving quadratic and nonlinear terms for gas-phase reactions, grain surface chemistry, freeze-out, and desorption processes
- **Stiff equations** requiring small timesteps for numerical stability

Direct integration of these ODEs is prohibitively slow for large-scale simulations (e.g., 3D hydrodynamic models with millions of cells). Our surrogate models provide speedups of several orders of magnitude while maintaining physical accuracy.

### Dataset Generation Workflow

Training data is generated by coupling astrochemical networks (UCLCHEM, see summary at bottom of this file) to hydrodynamic simulations:

1. Run hydrodynamic simulation (e.g., spherical gravitational collapse for star formation)
2. Place Lagrangian tracer particles throughout the simulation volume
3. Extract physical conditions (density, temperature, radiation field, visual extinction) along each tracer's trajectory
4. Post-process chemistry: run UCLCHEM for each tracer to compute chemical abundances evolution given the physical trajectory
5. Store physical parameters and abundances as time-series data in HDF5 format

This approach captures realistic correlations between physical conditions and chemistry across diverse astrophysical environments.

### Mathematical Formulation

For each Lagrangian tracer $j$ in the simulation:

- **Physical parameter sequence**: $\{\mathbf{p}_{j,0}, \mathbf{p}_{j,1}, \ldots, \mathbf{p}_{j,T}\}$ where $\mathbf{p}_{j,i} \in \mathbb{R}^4$ contains:
  - $\log_{10}(\text{density})$, $\log_{10}(\text{radfield})$, $\log_{10}(A_V)$, $\log_{10}(\text{gasTemp})$

- **Chemical abundance sequence**: $\{\mathbf{x}_{j,0}, \mathbf{x}_{j,1}, \ldots, \mathbf{x}_{j,T}\}$ where $\mathbf{x}_{j,i} \in \mathbb{R}^{333}$ represents species abundances relative to H nuclei

**Post-processing workflow:**

1. Run the entire hydrodynamic simulation to generate all physical trajectories: $\{\mathbf{p}_{j,i}\}$ for all tracers $j$ and timesteps $i$. We generate chemistry for 9989 tracers for 298 timesteps.
2. For each tracer, initialize chemistry with standard initial abundances: $\mathbf{x}_{j,0} = \mathbf{x}_{\text{init}}$ (stored in `utils/`)
3. Sequentially integrate chemistry using UCLCHEM:
   - $\mathbf{x}_{j,0}, \mathbf{p}_{j,0} \rightarrow \mathbf{x}_{j,1}$ (integrate from $t_0$ to $t_1$)
   - $\mathbf{x}_{j,1}, \mathbf{p}_{j,1} \rightarrow \mathbf{x}_{j,2}$ (integrate from $t_1$ to $t_2$)
   - Continue through $t_T$

**Learning objective:**

Train a neural network surrogate to replace the expensive UCLCHEM integration:
$$\mathbf{x}_{j,i+1} \approx f(\mathbf{p}_{j,i}, \mathbf{x}_{j,i}; \theta)$$

where $f$ provides orders-of-magnitude speedup over direct ODE integration.

### Model Architecture

AstroChemNet uses a two-stage deep learning architecture:

1. **Autoencoder** - Compresses chemical species abundances (333 species) into a lower-dimensional latent space (typically 12-14 dimensions)
   - Handles the extreme dynamic range via log-scaling
   - Tied-weight architecture for parameter efficiency

2. **Emulator** - A sequential model that predicts evolution of latent representations over time given physical parameters
   - Learns temporal dynamics in the compressed latent space
   - Autoregressive prediction enables long-term integration

## Development Commands

### Setup
```bash
pip install -e .
pip install -r requirements.txt
```

The package provides CLI commands for training models after installation. If you've previously installed the package, reinstall to register the CLI commands:
```bash
pip install -e . --force-reinstall --no-deps
```

### Training Models

After installing the package, use the CLI commands:

Train the autoencoder first (required before training emulator):
```bash
astrochemnet-train-autoencoder
```

Train the emulator (requires pretrained autoencoder):
```bash
astrochemnet-train-emulator
```

Override config parameters from command line:
```bash
astrochemnet-train-autoencoder model.lr=1e-4 model.batch_size=32768
astrochemnet-train-emulator device=cpu model.window_size=128
```

Change dataset or model variant:
```bash
astrochemnet-train-autoencoder dataset=grav model=autoencoder_large
astrochemnet-train-emulator dataset=turbulent
```

**Preprocessing and Training Modes:**

You can control whether to preprocess data, train, or do both with the `mode` flag:

```bash
# Only preprocess data (saves to disk)
astrochemnet-train-autoencoder mode=preprocess
astrochemnet-train-emulator mode=preprocess

# Only train (loads preprocessed data from disk)
astrochemnet-train-autoencoder mode=train
astrochemnet-train-emulator mode=train

# Both preprocess and train (default)
astrochemnet-train-autoencoder mode=both
astrochemnet-train-emulator  # mode=both is default
```

This is useful for:
- Preprocessing large datasets once, then running multiple training experiments
- Debugging preprocessing separately from training
- Running preprocessing on a different machine than training

**Preprocessed Data Locations:**
- Autoencoder: `data/autoencoder_train_preprocessed.pt`, `data/autoencoder_val_preprocessed.pt`
- Emulator: `data/training_seq.h5`, `data/validation_seq.h5`

### Code Quality

Lint and auto-fix code:
```bash
ruff check --fix
```

Format notebooks and Python files:
```bash
ruff format
```

### Code Style Guidelines

When writing or modifying code, follow these principles:

**Function Design:**
- Keep functions compact: **25 lines or fewer** (may exceed slightly if it improves visual clarity)
- Use **one-line docstrings** for most functions (e.g., `"""Compute latent space reconstruction loss."""`)
- Include **type hints** for all function arguments and return values

**Code Organization:**
- Write a **main function** that orchestrates the workflow by calling smaller helper functions
- The main function should read like a high-level outline of the algorithm
- Break complex logic into focused, single-purpose functions

**Documentation:**
- Prefer **clean, self-documenting code** over excessive comments
- Only add comments for genuinely complex or non-obvious logic
- Avoid comment clutter—the code should speak for itself

**Style:**
- Minimalistic and compact
- Prioritize readability through structure, not verbosity

**Example:**
```python
def train_epoch(model: nn.Module, loader: DataLoader, optimizer: optim.Optimizer) -> float:
    """Train model for one epoch and return average loss."""
    total_loss = 0.0
    for batch in loader:
        optimizer.zero_grad()
        loss = compute_loss(model, batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)
```

## Architecture Details

### Two-Stage Model Pipeline

The system uses a decoupled approach where the autoencoder learns to compress/decompress chemical abundances, and the emulator learns temporal evolution in the compressed latent space:

1. **Autoencoder** (`nn_architectures/autoencoder.py`):
   - Encoder: 333 → 320 → 160 → latent_dim (typically 12-14)
   - Decoder: Tied weights (transposed encoder weights)
   - Batch normalization after each layer
   - GELU activation, Sigmoid output activation
   - Training noise injection in latent space for regularization

2. **Emulator** (`nn_architectures/emulator.py`):
   - Sequential predictor that takes physical parameters + current latent state
   - Predicts next latent state via residual updates
   - Input: `[phys_params, latent_state]` → Output: `latent_update`
   - Autoregressive: `latent[t+1] = latent[t] + emulator(phys[t], latent[t])`

### Configuration System

**Uses Hydra for configuration management** with structured YAML configs and CLI overrides.

#### Configuration Structure

```
configs/
├── config.yaml              # Main config with defaults
├── datasets/
│   └── grav.yaml           # Dataset config (physical params, paths, species)
└── models/
    ├── autoencoder.yaml    # Autoencoder hyperparameters
    └── emulator.yaml       # Emulator hyperparameters
```

#### Structured Config Schemas

Defined in `src/AstroChemNet/config_schemas.py`:

- **DatasetConfig** - Dataset paths, physical parameter ranges, device settings, species lists (replaces old `GeneralConfig`)
- **ModelsConfig** - Single reusable model schema for both autoencoder and emulator; uses `Optional` fields for model-specific parameters
- **Config** - Top-level composition of `dataset` and `model` configs

#### Config Access Pattern

Training scripts receive OmegaConf DictConfig objects:
```python
@hydra.main(config_path="../../configs", config_name="config", version_base=None)
def main(cfg: DictConfig):
    device = cfg.dataset.device
    lr = cfg.model.lr
    batch_size = cfg.model.batch_size
```

Computed fields (e.g., `num_species`, loaded numpy arrays) are added via `setup_config()` helper in training scripts.

#### CLI Overrides

Override any config value from command line:
```bash
python train_autoencoder.py model.lr=5e-4 model.dropout=0.2
python train_emulator.py dataset=grav model=emulator model.window_size=128
```

### Data Flow

1. Raw data loaded from HDF5 (`data_loading.load_datasets()`)
2. Abundances clipped to [1e-20, 1.0] range
3. Scaled via `Processing.abundances_scaling()` (log10 transformation)
4. Autoencoder trained on scaled abundances
5. Latent representations computed and scaled via `Processing.save_latents_minmax()`
6. Emulator trained on sequences in latent space

### Custom Loss Functions

Located in `src/AstroChemNet/loss.py`:

- **Training Loss**: Combines reconstruction error, worst-species penalty, and elemental conservation
  - Exponential weighting emphasizes larger errors
  - Conservation loss: ensures elemental abundances are preserved (via stoichiometric matrix)

- **Validation Loss**: Species-wise mean relative error
  - Returns per-species loss vector (333 elements)
  - Used for early stopping and learning rate scheduling

### Training Infrastructure

`src/AstroChemNet/trainer.py` contains base `Trainer` class with:
- Automatic early stopping based on validation stagnation
- Adaptive dropout rate reduction when training stagnates
- Learning rate scheduling (ReduceLROnPlateau)
- Gradient clipping
- Best model checkpoint saving
- Detailed per-epoch metrics logging to JSON

Specialized trainers:
- `AutoencoderTrainer` - Standard reconstruction training
- `EmulatorTrainerSequential` - Sequential prediction with autoregressive rollout

### Data Loading Optimizations

`src/AstroChemNet/data_loading.py` implements efficient batching:

- `ChunkedShuffleSampler`: Shuffles data in chunks to manage large datasets
- `__getitems__` batching: Loads entire batch in single call (10^3x faster than individual loads)
- `EmulatorSequenceDataset`: Memory-efficient sequence dataset using index arrays
- Pin memory enabled for GPU transfer

## Key Implementation Details

### Physical Parameters

The model tracks 4 physical parameters (defined in `configs/datasets/grav.yaml`):
- Density: H nuclei per cm³
- Radfield: Habing field (radiation field strength)
- Av: Visual extinction in magnitudes
- gasTemp: Gas temperature in Kelvin

All physical parameters are sampled and stored in log space.

### Species Abundances

- 333 chemical species tracked (list in `utils/species.txt`)
- All abundances relative to H nuclei (max = 1.0)
- Clipped to [1e-20, 1.0] before processing
- Log10 scaling applied for training

### Stoichiometric Matrix

Matrix stored in `utils/stoichiometric_matrix.npy` maps species abundances to elemental abundances. Used in conservation loss to ensure physical validity (atoms can't be created/destroyed).

### Pretrained Model Loading

Both model classes have `load_autoencoder()` and `load_emulator()` functions:
- Check for pretrained weights at configured path
- Set `inference=True` to freeze weights and set eval mode
- Models default to CPU unless CUDA available

### Weight Tying

Autoencoder uses tied weights (decoder reuses transposed encoder weights) to:
- Reduce parameter count
- Improve generalization
- Enforce symmetry in latent space

## File Organization

### Core Package (`src/AstroChemNet/`)
- `cli.py` - Command-line interface entry points for training
- `config_schemas.py` - Hydra structured config dataclasses
- `trainer.py` - Training loops and optimization logic
- `loss.py` - Custom loss functions with conservation constraints
- `data_loading.py` - HDF5 loading, Dataset/DataLoader implementations
- `data_processing.py` - Scaling, normalization, preprocessing utilities
- `inference.py` - Inference pipelines and evaluation
- `analysis.py` - Plotting and metrics helpers
- `utils.py` - Shared helper functions

### Model Definitions (`nn_architectures/`)
- `autoencoder.py` - Tied-weight autoencoder with batch norm
- `emulator.py` - Sequential latent dynamics predictor

### Experiments
- `vibecode/` - ViBe baseline experiments and comparison scripts
- `research/` - Exploratory Jupyter notebooks
- `scripts/analysis/` - Post-training analysis notebooks
- `scripts/train/` - **Deprecated** training scripts (use CLI commands instead)

### Data Storage
- `data/` - Local HDF5 datasets (not in repo, download separately)
- `weights/` - Model checkpoints (archived_original/ contains reference models)
- `utils/` - Cached arrays (species list, stoichiometric matrix, initial abundances)

## Notes on Dependencies

The `requirements.txt` file appears to have encoding issues. Expected dependencies:
- PyTorch (with CUDA 12.6 support)
- NumPy, Pandas, SciPy
- H5py, tables (HDF5 support)
- Matplotlib, Seaborn (visualization)
- tqdm (progress bars)
- Numba (JIT compilation)

## Common Workflows

### Training a New Model

1. Ensure dataset exists at path specified in `configs/datasets/grav.yaml`
2. Configure hyperparameters by editing YAML files in `configs/models/` or via CLI overrides
3. Train autoencoder first: `astrochemnet-train-autoencoder`
4. After autoencoder converges, train emulator: `astrochemnet-train-emulator`

Example with hyperparameter tuning:
```bash
astrochemnet-train-autoencoder model.lr=5e-4 model.dropout=0.25 model.batch_size=32768
```

### Evaluating Models

Use `src/AstroChemNet/inference.py` which provides the `Inference` class for:
- Encoding abundances to latent space
- Decoding latent representations to abundances
- Running emulator predictions

### Adding New Datasets

1. Create new YAML file in `configs/datasets/` (e.g., `turbulent.yaml`)
2. Define dataset path, physical parameter ranges, and species list
3. Train with: `astrochemnet-train-autoencoder dataset=turbulent`

### Adding New Model Variants

1. Create new YAML file in `configs/models/` (e.g., `autoencoder_large.yaml`)
2. Define architecture and hyperparameters (reuses `ModelsConfig` schema)
3. Train with: `astrochemnet-train-autoencoder model=autoencoder_large`

### Adding New Species or Physical Parameters

1. Update dataset YAML in `configs/datasets/` with new parameter ranges/species path
2. Regenerate `utils/stoichiometric_matrix.npy` if species changed
3. Update `input_dim` in model YAML to match new dimensions
4. Hydra will handle config composition automatically



### UCLCHEM Summary
# UCLCHEM Summary

## Cloud Model

The UCLCHEM cloud model is a 1D spherical cloud simulator that models positions evenly spaced between an inner radius (rin) and outer radius (rout), with each position treated almost independently except that outer positions calculate column densities for inner positions to handle self-shielding of H₂, C, and CO. It uses constant physical conditions (except density, which can vary via the freefall parameter) and fixed temperature throughout, making it suitable for UV-shielded molecular cloud cores (typically run with points=1 and high Av) or for generating initial abundances by collapsing low-density atomic gas to desired densities. However, it's not a proper PDR model since it lacks heating/cooling calculations and insufficient resolution in low-Av regions, so 3D-PDR is recommended for photodissociation region studies.

## Equations

### Gas Phase Reactions

**Two-body (concentration):** $dn_i/dt = k_{ij} n_i n_j$

**Two-body (fractional):** $dx_i/dt = k_{ij} x_i x_j n_H$

**Single-body:** $dx_i/dt = k_i x_i$

**Rate formulas:**
- Kooij-Arrhenius: $k = \alpha (T/300)^\beta \exp(-\gamma/T)$
- CR protons: $k = \alpha (\zeta/1.3 \times 10^{-17})$
- CR photons: $k = \alpha (\zeta/1.3 \times 10^{-17}) (T/300)^\beta \gamma/(1-\omega)$
- UV photons: $k = \alpha \exp(-\gamma A_V) \chi$

### Grain Surface Reactions

**Langmuir-Hinshelwood:** $dx_k/dt = \kappa_{ij} (k_{\text{diff},i} k_{\text{diff},j}/N_{\text{sites}}) x_i x_j n_H$

**Eley-Rideal:** $dx_k/dt = k_{\text{freeze},i} P(\gamma) x_j$

### Adsorption & Desorption

**Freeze-out:** $k_{\text{freeze}} = \alpha \langle v \rangle \sigma_d$

**Thermal desorption:** $k_{\text{evap}} = \nu_0 \exp(-E_{\text{bind}}/k_B T)$

**Two-phase thermal:** $dx_i/dt = -k_{\text{evap}} (x_i/x_{\text{ice,tot}}) N_{\text{sites}} n_d/n_H$

### Bulk Ice Processes

**Surface transfer:** $dx_{i,\text{bulk}}/dt = (\sum_j dx_{j,\text{surf}}/dt) \cdot (x_{i,\text{surf}}/x_{\text{surf,tot}})$

**Bulk→Surface swap:** $dx_{i,\text{surf}}/dt = (k_{\text{diff},i}/N_{\text{layers}}) x_{i,\text{bulk}}$

**Surface→Bulk swap:** $dx_{i,\text{bulk}}/dt = (\sum_j k_{\text{diff},j} x_{j,\text{bulk}}/N_{\text{layers}}) \cdot (x_{i,\text{surf}}/x_{\text{surf,tot}})$

## Notation

- Gas phase: `H2O`
- Surface: `#H2O`
- Bulk: `@H2O`
- Total ice: `$H2O` (surface + bulk)
